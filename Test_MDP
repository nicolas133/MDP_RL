from mdp_dp import *
import gym
import numpy as np






Race_Track= [
 'G' 'F' 'F' 'H' 'H' 'F' 'H',
 'F' 'F' 'H' 'H' 'H' 'F' 'H',
 'F' 'F' 'H' 'H' 'H' 'F' 'F',
 'F' 'F' 'F' 'F' 'F' 'F' 'F',
 'F' 'F' 'F' 'F' 'F' 'F' 'F',
 'H' 'F' 'F' 'F' 'F' 'F' 'F',
 'H' 'F' 'H' 'H' 'H' 'H' 'F',
 'H' 'F' 'H' 'H' 'H' 'H' 'S',
]

gym.envs.register(
    id='FrozenLake8x7NotSlippery-v0',
    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',
    kwargs={'desc': Race_Track, 'is_slippery': False},  # Ensure it's not slippery

)
env = gym.make('FrozenLake8x7NotSlippery-v0')

nS = env.unwrapped.nS
nA = env.unwrapped.nA
P = env.unwrapped.P


policy = np.ones([nS, nA]) / nA

#policy iteration
new_policy, V = policy_iteration(P, nS, nA, policy, gamma=0.9, tol=1e-8)

# value iteration
new_policy2, V2 = value_iteration(P, nS, nA, policy, gamma=0.9, tol=1e-8)



actions = ["left", "down", "right", "up"]

env.render()

# Open a GUI window to visualize the environment
env.reset()
env.render(mode='human')

# Play the game using the policy obtained from policy iteration
done = False
while not done:
    action = np.argmax(new_policy[env.s])
    observation, reward, done, info = env.step(action)
    env.render(mode='human')


for state, action_probs in enumerate(new_policy):
    action = np.argmax(action_probs)
    print(action_probs)
    print(f"State {state} should take action {actions[action]}")
#print(f"Value Function from Policy Iteration: {V}")
#print(f"Policy from Value Iteration: {new_policy2}")
#print(f"Value Function from Value Iteration: {V2}")
