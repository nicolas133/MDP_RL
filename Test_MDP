from mdp_dp import *
import gym
import numpy as np



Race_Track= [
 'G' 'F' 'F' 'H' 'H' 'F' 'H',
 'F' 'F' 'H' 'H' 'H' 'F' 'H',
 'F' 'F' 'H' 'H' 'H' 'F' 'F',
 'F' 'F' 'F' 'F' 'F' 'F' 'F',
 'F' 'F' 'F' 'F' 'F' 'F' 'F',
 'H' 'F' 'F' 'F' 'F' 'F' 'F',
 'H' 'F' 'H' 'H' 'H' 'H' 'F',
 'H' 'F' 'H' 'H' 'H' 'H' 'S',
]

gym.envs.register(
    id='FrozenLake8x7NotSlippery-v0',a
    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',
    kwargs={'desc': Race_Track, 'is_slippery': False},  # Ensure it's not slippery

)
env = gym.make('FrozenLake8x7NotSlippery-v0')

nS = env.unwrapped.nS
nA = env.unwrapped.nA
P = env.unwrapped.P


policy = np.ones([nS, nA]) / nA

#policy iteration
new_policy, V = policy_iteration(P, nS, nA, policy, gamma=0.9, tol=1e-8)

# value iteration
new_policy2, V2 = value_iteration(P, nS, nA, policy, gamma=0.9, tol=1e-8)

# list of actions from the best policy
optimal_action_list = []

actions = ["left", "down", "right", "up"]

env.render()

# Open a GUI window to visualize the environment
env.reset()
env.render(mode='human')

# Play the game using the policy obtained from policy iteration
# done = False
# while not done:
#     action = np.argmax(new_policy[env.s])
#     observation, reward, done, info = env.step(action)
#     env.render(mode='human')

# print(new_policy)

# for state, action_probs in enumerate(new_policy):
#     action = np.argmax(action_probs)
#     print(action_probs)
#     print(f"State {state} should take action {actions[action]}")


# Play the game using the policy obtained from value iteration
# print(f"Value Function from Policy Iteration: {V}")

# 0: LEFT

# 1: DOWN

# 2: RIGHT

# 3: UP
done = False
while not done:
    # locate and perform the best action from value_iteraiton's policy
    action = np.argmax(new_policy2[env.s])
    ob, reward, done, info = env.step(action)

    # store every action in an array and print the environment
    optimal_action_list.append(action)
    env.render(mode='human')

for state_counter, action_probs in enumerate(new_policy2):
    action = np.argmax(action_probs)
    print(action_probs)
    print(f"State {state_counter} should take action {actions[action]}")
    #print(f"Expected value for state {state_counter} is  {V2[state_counter]}")

# prints the list of actions R2 will take
print(f"Optimal Action List = {optimal_action_list}")
