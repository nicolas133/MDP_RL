from mdp_dp import *
import gym
import numpy as np






Race_Track= [
 'G' 'F' 'F' 'H' 'H' 'F' 'H',
 'F' 'F' 'H' 'H' 'H' 'F' 'H',
 'F' 'F' 'H' 'H' 'H' 'F' 'F',
 'F' 'F' 'F' 'F' 'F' 'F' 'F',
 'F' 'F' 'F' 'F' 'F' 'F' 'F',
 'H' 'F' 'F' 'F' 'F' 'F' 'F',
 'H' 'F' 'H' 'H' 'H' 'H' 'F',
 'H' 'F' 'H' 'H' 'H' 'H' 'S',
]

gym.envs.register(
    id='FrozenLake8x7NotSlippery-v0',
    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',
    kwargs={'desc': Race_Track, 'is_slippery': False},  # Ensure it's not slippery

)
env = gym.make('FrozenLake8x7NotSlippery-v0')

nS = env.unwrapped.nS
nA = env.unwrapped.nA
P = env.unwrapped.P


policy = np.ones([nS, nA]) / nA

#policy iteration
new_policy, V = policy_iteration(P, nS, nA, policy, gamma=0.9, tol=1e-8)

# value iteration
new_policy2, V2 = value_iteration(P, nS, nA, policy, gamma=0.9, tol=1e-8)

# list of actions from the best policy
optimal_action_list = []

actions = ["left", "down", "right", "up"]

env.render()

# Open a GUI window to visualize the environment
env.reset()
env.render(mode='human')

# Play the game using the policy obtained from policy iteration
# done = False
# while not done:
#     action = np.argmax(new_policy[env.s])
#     observation, reward, done, info = env.step(action)
#     env.render(mode='human')

# print(new_policy)

# for state, action_probs in enumerate(new_policy):
#     action = np.argmax(action_probs)
#     print(action_probs)
#     print(f"State {state} should take action {actions[action]}")


# Play the game using the policy obtained from value iteration
# print(f"Value Function from Policy Iteration: {V}")

# 0: LEFT

# 1: DOWN

# 2: RIGHT

# 3: UP
done = False
while not done:
    # locate and perform the best action from value_iteraiton's policy
    action = np.argmax(new_policy2[env.s])
    ob, reward, done, info = env.step(action)

    # store every action in an array and print the environment
    optimal_action_list.append(action)
    env.render(mode='human')

for state_counter, action_probs in enumerate(new_policy2):
    action = np.argmax(action_probs)
    print(action_probs)
    print(f"State {state_counter} should take action {actions[action]}")
    #print(f"Expected value for state {state_counter} is  {V2[state_counter]}")

# prints the list of actions R2 will take
print(f"Optimal Action List = {optimal_action_list}")

# prints the action performed by the Rosmaster R2
def sendAction(action):
    if action == 0:
        print(f"R2 will perform this action: Turn Left")
    if action == 1:
        print(f"R2 will perform this action: Drive Backward")
    if action == 2:
        print(f"R2 will perform this action: Turn Right")
    if action == 3:
        print(f"TTTTTTTTTTTTTTTTR2 will perform this action: Drive Forward")

orientation_count = 3
# alters actions for input on the Rosmaster R2 based on orientation
for action in optimal_action_list:
    print(f"CURRENTLY READING ACTION = {action}")
    # Assume start facing North
    # this loops back to 0 once you've made a full revolution
    orientation_count = orientation_count%4
    # this loops back to 0 once you've made a full revolution
    orientation = orientation_count%4
    print(f"orientation count = {orientation_count}")


    # if turning left or right, switch orientation by rearranding the values of O_A_L
    face_N = {0:0, 1:1, 2:2, 3:3}
    face_W = {0:3, 1:0, 2:1, 3:2}
    face_S = {0:2, 1:3, 2:0, 3:1}
    face_E = {0:1, 1:2, 2:3, 3:0}


    # perform your action based on your current orientation - corresponds with L,D,R,U = 0,1,2,3
    if orientation == 0:
        # change orientation count LEFT
        if face_W[action] == 0:
            orientation_count+=1
        # change orientation count right
        if face_W[action] == 2:
            orientation_count-=1

        sendAction(face_W[action])
        print(f"I am currently facing West!\n")

    if orientation == 1:
        # change orientation count LEFT
        if face_S[action] == 0:
            orientation_count+=1
        # change orientation count RIGHT
        if face_S[action] == 2:
            orientation_count-=1

        sendAction(face_S[action])
        print(f"I am currently facing South!\n")

    if orientation == 2:
        # change orientation count LEFT
        if face_E[action] == 0:
            orientation_count+=1
        # change orientation count RIGHT
        if face_E[action] == 2:
            orientation_count=-1

        sendAction(face_E[action])
        print(f"I am currently facing East!\n")

    if orientation == 3:
        # change orientation count LEFT
        if face_N[action] == 0:
            orientation_count+=1
        # change orientation count RIGHT
        if face_N[action] == 2:
            orientation_count-=1

        sendAction(face_N[action])
        print(f"I am currently facing North!\n")


    # schmubbber



#print(f"action value map: {V2}")
#print(f"These are the actions taken: {optimal_action_list}")



#print(f"Policy from Value Iteration: {new_policy2}")
#print(f"Value Function from Value Iteration: {V2}")
